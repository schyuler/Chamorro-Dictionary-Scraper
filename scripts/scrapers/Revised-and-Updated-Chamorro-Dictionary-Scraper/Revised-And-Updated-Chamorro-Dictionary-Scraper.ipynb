{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6b18b8-5d37-4607-8e08-98af81f075d6",
   "metadata": {},
   "source": [
    "# Revised and Updated Chamorro Dictionary Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86e3dc-5ad3-43d6-8067-ac030c397dd0",
   "metadata": {},
   "source": [
    "**About This Notebook**<br>\n",
    "This notebook contains the full processing pipeline for scraping, parsing, formatting and exporting the contents of the Revised and Updated Chamorro-English Dictionary found at https://natibunmarianas.org/chamorro-dictionary/ which is a project to revise and update the Chamorro-English Dictionary by Topping, Ogo, and Dungca (known as the \"TOD\" Dictionary).\n",
    "\n",
    "This notebook includes functions for:\n",
    "\n",
    "* Scraping the contents from the online dictionary\n",
    "* Parsing and formatting the contents based on html tags\n",
    "* Structuring the data into a nested Python dictionary\n",
    "* Exporting the formatted data to JSON for future use\n",
    "* Exporting the example sentences to CSV for use in a Chamorro corpus\n",
    "\n",
    "The goal of this notebook is to prepare the dictionary data for easier analysis, transformation, or integration into future linguistic, educational, or machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3032bcb-aa13-44f2-b4df-d6828c2769de",
   "metadata": {},
   "source": [
    "**Name:** Schyuler Lujan <br>\n",
    "**Date Started:** 14-May-2025 <br>\n",
    "**Date Completed:** In Progress <br>\n",
    "**Date Updated:** 15-May-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06940e4-da5d-4e3a-a703-c308054f5ec9",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "3e184ed2-84c3-4f94-a2d9-9e966f728a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f656978-0fee-4830-b786-8d595dd8d946",
   "metadata": {},
   "source": [
    "# Scrape, Parse and Extract Dictionary Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5445be-35e0-49d7-8b2c-b8809217dbbd",
   "metadata": {},
   "source": [
    "In this section we will use `BeautifulSoup` to scrape, parse, extract and format the dictionary contents on the website. Each term is wrapped in a `<p>` paragraph tag with `class=\"EntryParagraph\"`. The formatting of the HTML allows us to easily target the following elements for scraping and extraction:\n",
    "\n",
    "* **Term:** class=\"Lexeme\"\n",
    "* **Part of Speech:** class=\"Partofspeech\"\n",
    "* **Definition:** class=\"DefinitionE\"\n",
    "\n",
    "**Other Contents (Example Sentences, Synonyms, etc.)**<br>\n",
    "To scrape and extract the other contents associated with a word, such as Chamorro example sentences, their English translations, synonyms, scientific names, loanword designations, etc. we will be scraping all of the text content associated with a term, and then performing text cleanup.\n",
    "\n",
    "**Handling Words With More Than One Part of Speech**<br>\n",
    "The website also distinguishes terms that have more than one Part of Speech. Visually on the website they appear as follows:\n",
    "\n",
    "    abråsa n. unit of measurement from fingertip of outstretched arm to opposite shoulder. Un abråsa ha' inanakko'‑ña i tali. The length of the rope is about a yard long.\n",
    "\n",
    "    — vt. encompass, cover a certain area. Ha abråsa todu i manmåolik na chå'guan i kellat guaka. The fenced-in area for cattle encompassed all the good grass. I kellat ha abråsa todu i sitiun i gima'. The fence covered all the area around the house. From: Sp. abraza.\n",
    "\n",
    "In these instances, the second entry does not include a term. It is also wrapped in a separate `<p>` paragraph tag with `class=\"BlockParagraph\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477010fb-2c0c-4599-aa0c-696b5cab76f1",
   "metadata": {},
   "source": [
    "## Create URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c87ab6-eaff-4d64-84fd-e98578374fd9",
   "metadata": {},
   "source": [
    "First we create the URLs that we will navigate to for scraping and parsing the dictionary content. The website has a different webpage for each letter of the Chamoru alphabet, and we will construct the URLs according to this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "81f9a2f4-cef7-4118-b9fe-d8d9bcf0134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the urls for the website\n",
    "letters = [\n",
    "    \"a-2\", \"b\", \"ch\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"k\", \"l\", \"m\", \n",
    "    \"n\", \"n-2\", \"ng\", \"o\", \"p\", \"r\", \"s\", \"t\", \"u\", \"y\"\n",
    "]\n",
    "\n",
    "main_url = \"https://natibunmarianas.org/\"\n",
    "\n",
    "# Create list of urls\n",
    "page_urls = [main_url+letter+\"/\" for letter in letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69373f58-9743-4f7c-835a-513143ee76e1",
   "metadata": {},
   "source": [
    "## Get Dictionary Content From Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "aaab7c62-bb9d-4a57-96d1-071922dfd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary_content(urls):\n",
    "    \"\"\"\n",
    "    Navigates to each dictionary page on the website, parses the HTML and extracts the contents based on the tags and returns results\n",
    "    in a dictionary.\n",
    "    \"\"\"\n",
    "    ### FIXME: Handle class_=\"BlockParagraph\" ###\n",
    "    \n",
    "    # Initialize dictionary for storing contents\n",
    "    dictionary = {}\n",
    "\n",
    "    # Set headers to avoid 406 error\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "    }\n",
    "\n",
    "    # Go to the url and parse the HTML\n",
    "    for url in urls:\n",
    "        # Request webpage\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Iterate through tags, extract the text and store in a Python dictionary\n",
    "        for entry in soup.find_all(\"p\", class_=\"EntryParagraph\"):\n",
    "            word = None # Initialize variable to store the word\n",
    "            lexeme_tag = entry.find(class_=\"Lexeme\") # Find the tag for the word\n",
    "    \n",
    "            # Get the word and add to the dictionary\n",
    "            if lexeme_tag:\n",
    "                word = lexeme_tag.get_text(strip=True)\n",
    "                dictionary[word] = {}\n",
    "    \n",
    "                ### PART OF SPEECH ###\n",
    "                # Get part of speech text\n",
    "                pos_tag = entry.find(class_=\"Partofspeech\")\n",
    "                dictionary[word][\"PartOfSpeech\"] = pos_tag.get_text(strip=True) if pos_tag else \"n/a\"\n",
    "\n",
    "                ### DEFINITION ###\n",
    "                # Get Definition tag and text\n",
    "                def_tag = entry.find(class_=\"DefinitionE\")\n",
    "                def_text = def_tag.get_text() if def_tag else \"n/a\"\n",
    "                \n",
    "                # Text Cleanup\n",
    "                def_text = def_text.replace(\"\\r\", \" \").replace(\"\\n\",\" \")\n",
    "                def_text = def_text.replace(\"\\xa0\", \" \").replace(\"\\'\", \"\")\n",
    "                \n",
    "                # Add definition to dictionary\n",
    "                dictionary[word][\"Definition\"] = def_text\n",
    "\n",
    "                ### OTHER CONTENTS ###\n",
    "                # Get full text -- will make text cleanup easier\n",
    "                full_text = entry.get_text(separator=\" \")\n",
    "\n",
    "                # Text cleanup\n",
    "                full_text = full_text.replace(\"\\r\", \" \").replace(\"\\n\",\" \") # Remove tags\n",
    "                full_text = full_text.replace(\"\\xa0\", \" \").replace(\"\\'\", \"'\") # Remove tags\n",
    "                full_text = full_text.replace(def_text, \" \") # Remove definition\n",
    "                full_text = full_text.split(\".\") # Split to prep for text cleanup on each sentence\n",
    "\n",
    "                # Cleanup each string and append to a list\n",
    "                cleaned_text = [string.lstrip().rstrip() for string in full_text]\n",
    "                cleaned_text.pop(0) # Remove term and pos strings\n",
    "                \n",
    "                # Add cleaned text to dictionary\n",
    "                dictionary[word][\"Other\"] = cleaned_text\n",
    "\n",
    "        time.sleep(3)\n",
    "            \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "a25cfeab-0ba4-40e8-b202-0718bbd8de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary contents\n",
    "contents = get_dictionary_content(page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fcd436-4343-4cf3-9cc9-bc28759ccd09",
   "metadata": {},
   "source": [
    "# Extract and Format Example Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13f1e24e-d4c7-45d4-992e-8768cd1354a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_sentences(dictionary_contents):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # TODO Extract example sentences from formatted Python dictionary\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6a607-2333-4dec-856b-3f9917096ada",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bca72-02d8-49a7-a6a4-ac318d321299",
   "metadata": {},
   "source": [
    "In this section we will export the dictionary contents and example sentences for future analysis and use in other language projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "03b00efe-060f-4b52-a3b4-e1d05097a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the absolute path for exporting data\n",
    "base_path = # ABSOLUTE PATH GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95876be-bb2c-4a66-9be1-a1611ba21155",
   "metadata": {},
   "source": [
    "## Export Dictionary to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "2d04a658-bdd1-49af-bc33-32200d9d5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to regular Python dictionary\n",
    "regular_dict = {key: value for key, value in contents.items()}\n",
    "\n",
    "# Set folder path and filename\n",
    "folder_path = base_path + \"Chamorro-Dictionary-Scraper/exports/json\"\n",
    "filename = \"revised_and_updated_chamorro_dictionary.json\"\n",
    "\n",
    "# Set file path for export\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Export to JSON\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(regular_dict, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb013e-ad15-4e6d-85a5-86fbbc1485b0",
   "metadata": {},
   "source": [
    "## Export Example Sentences to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "680166ce-c34d-4e0b-93cf-309cf9b1e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert to dataframe\n",
    "\n",
    "# TODO Set folder path and filename\n",
    "\n",
    "# TODO Set file path for export\n",
    "\n",
    "# TODO Export to CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
